# @package _global_

ssl:
  model_name: "microsoft/wavlm-large"
  layer: 24

rqvae:
  latent_dim: 1024
  codebook_size: 256
  num_codebooks: 8
  commitment_cost: 0.25
  decay: 0.9
  num_encoder_layers: 4
  num_decoder_layers: 4

data:
  manifest_path: ???
  val_manifest: null
  max_length: 160000 # 10 seconds
  num_workers: 4

training:
  batch_size: 32
  lr: 1e-4
  epochs: 50
  save_steps: 1000

logging:
  project: "speechgr-rqvae"
  name: "rqvae_transformer_ema"
  mode: "online" # set to "disabled" to skip wandb

# Monitoring configuration
monitoring:
  enabled: true
  enable_codebook: true
  enable_ema: true
  enable_recon: true
  enable_training: true
  enable_alerts: true
  summary_interval: 500  # Print summary every N steps
  alert_cooldown: 100    # Min steps between same alert type
  low_utilization_threshold: 0.3
  codebook_collapse_threshold: 0.08
  low_perplexity_ratio: 0.2
  poor_reconstruction_snr_db: 0.0
  normalize_recon_to_training_target: true
  stop_on_critical: false  # Set to true to stop training on critical alerts

hydra:
  run:
    dir: outputs/rqvae/${now:%Y-%m-%d}/${now:%H-%M-%S}
